{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tools import init_paths\n",
    "\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger\n",
    "\n",
    "import dataset\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying validate function\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from core.function import AverageMeter\n",
    "from core.function import _print_name_value\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils.transforms import flip_back\n",
    "from core.evaluate import accuracy\n",
    "from core.inference import get_final_preds\n",
    "from utils.vis import save_debug_images\n",
    "\n",
    "\n",
    "def my_validate(val_loader, val_dataset, model, criterion, output_dir,\n",
    "             tb_log_dir, writer_dict=None):\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        all_preds, all_boxes, losses,  acc, image_path, filenames, imgnums = predict_batch(val_loader,val_dataset,model,criterion,output_dir)\n",
    "\n",
    "        name_values, perf_indicator = val_dataset.evaluate(\n",
    "            cfg, all_preds, output_dir, all_boxes, image_path,\n",
    "            filenames, imgnums\n",
    "        )\n",
    "\n",
    "        model_name = cfg.MODEL.NAME\n",
    "        if isinstance(name_values, list):\n",
    "            for name_value in name_values:\n",
    "                _print_name_value(name_value, model_name)\n",
    "        else:\n",
    "            _print_name_value(name_values, model_name)\n",
    "\n",
    "        if writer_dict:\n",
    "            writer = writer_dict['writer']\n",
    "            global_steps = writer_dict['valid_global_steps']\n",
    "            writer.add_scalar(\n",
    "                'valid_loss',\n",
    "                losses.avg,\n",
    "                global_steps\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                'valid_acc',\n",
    "                acc.avg,\n",
    "                global_steps\n",
    "            )\n",
    "            if isinstance(name_values, list):\n",
    "                for name_value in name_values:\n",
    "                    writer.add_scalars(\n",
    "                        'valid',\n",
    "                        dict(name_value),\n",
    "                        global_steps\n",
    "                    )\n",
    "            else:\n",
    "                writer.add_scalars(\n",
    "                    'valid',\n",
    "                    dict(name_values),\n",
    "                    global_steps\n",
    "                )\n",
    "            writer_dict['valid_global_steps'] = global_steps + 1\n",
    "\n",
    "    return perf_indicator\n",
    "\n",
    "\n",
    "def predict_batch(val_loader, val_dataset, model, criterion, output_dir):\n",
    "    end = time.time()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    num_samples = len(val_dataset)\n",
    "    all_preds = np.zeros(\n",
    "            (num_samples, cfg.MODEL.NUM_JOINTS, 3),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    all_boxes = np.zeros((num_samples, 6))\n",
    "    image_path = []\n",
    "    filenames = []\n",
    "    imgnums = []\n",
    "    idx = 0\n",
    "    for i, (input, target, target_weight, meta) in enumerate(val_loader):\n",
    "            # compute output\n",
    "            outputs = model(input)\n",
    "            if isinstance(outputs, list):\n",
    "                output = outputs[-1]\n",
    "            else:\n",
    "                output = outputs\n",
    "\n",
    "            if cfg.TEST.FLIP_TEST:\n",
    "                # this part is ugly, because pytorch has not supported negative index\n",
    "                # input_flipped = model(input[:, :, :, ::-1])\n",
    "                input_flipped = np.flip(input.cpu().numpy(), 3).copy()\n",
    "                input_flipped = torch.from_numpy(input_flipped).cuda()\n",
    "                outputs_flipped = model(input_flipped)\n",
    "\n",
    "                if isinstance(outputs_flipped, list):\n",
    "                    output_flipped = outputs_flipped[-1]\n",
    "                else:\n",
    "                    output_flipped = outputs_flipped\n",
    "\n",
    "                output_flipped = flip_back(output_flipped.cpu().numpy(),\n",
    "                                           val_dataset.flip_pairs)\n",
    "                output_flipped = torch.from_numpy(output_flipped.copy()).cuda()\n",
    "\n",
    "\n",
    "                # feature is not aligned, shift flipped heatmap for higher accuracy\n",
    "                if cfg.TEST.SHIFT_HEATMAP:\n",
    "                    output_flipped[:, :, :, 1:] = \\\n",
    "                        output_flipped.clone()[:, :, :, 0:-1]\n",
    "\n",
    "                output = (output + output_flipped) * 0.5\n",
    "\n",
    "            target = target.cuda(non_blocking=True)\n",
    "            target_weight = target_weight.cuda(non_blocking=True)\n",
    "\n",
    "            loss = criterion(output, target, target_weight)\n",
    "\n",
    "            num_images = input.size(0)\n",
    "            # measure accuracy and record loss\n",
    "            losses.update(loss.item(), num_images)\n",
    "            _, avg_acc, cnt, pred = accuracy(output.cpu().numpy(),\n",
    "                                             target.cpu().numpy())\n",
    "\n",
    "            acc.update(avg_acc, cnt)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            c = meta['center'].numpy()\n",
    "            s = meta['scale'].numpy()\n",
    "            score = meta['score'].numpy()\n",
    "\n",
    "            preds, maxvals = get_final_preds(\n",
    "                cfg, output.clone().cpu().numpy(), c, s)\n",
    "\n",
    "            all_preds[idx:idx + num_images, :, 0:2] = preds[:, :, 0:2]\n",
    "            all_preds[idx:idx + num_images, :, 2:3] = maxvals\n",
    "            # double check this all_boxes parts\n",
    "            all_boxes[idx:idx + num_images, 0:2] = c[:, 0:2]\n",
    "            all_boxes[idx:idx + num_images, 2:4] = s[:, 0:2]\n",
    "            all_boxes[idx:idx + num_images, 4] = np.prod(s*200, 1)\n",
    "            all_boxes[idx:idx + num_images, 5] = score\n",
    "            image_path.extend(meta['image'])\n",
    "\n",
    "            idx += num_images\n",
    "\n",
    "            if i % cfg.PRINT_FREQ == 0:\n",
    "                msg = 'Test: [{0}/{1}]\\t' \\\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                      'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                          i, len(val_loader), batch_time=batch_time,\n",
    "                          loss=losses, acc=acc)\n",
    "                logger.info(msg)\n",
    "\n",
    "                prefix = '{}_{}'.format(\n",
    "                    os.path.join(output_dir, 'val'), i\n",
    "                )\n",
    "                save_debug_images(cfg, input, meta, target, pred*4, output,\n",
    "                                  prefix)\n",
    "    return all_preds, all_boxes, losses,  acc, image_path, filenames, imgnums \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_my_config():\n",
    "    cfg.defrost()\n",
    "\n",
    "    cfg.merge_from_file('experiments/mpii/hrnet/w32_256x256_adam_lr1e-3.yaml')\n",
    "\n",
    "    opts = [\"TEST.MODEL_FILE\", \"/mnt/models/HRNet/pose_mpii/pose_hrnet_w32_256x256.pth\"]\n",
    "    cfg.merge_from_list(opts)\n",
    "\n",
    "    cfg.OUTPUT_DIR = \"output_test\"\n",
    "\n",
    "    cfg.LOG_DIR = \"log_test\"\n",
    "\n",
    "    cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: mpii\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: -1.0\n",
      "  ROOT: data/mpii/\n",
      "  ROT_FACTOR: 30\n",
      "  SCALE_FACTOR: 0.25\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: valid\n",
      "  TRAIN_SET: train\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: True\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0, 1, 2, 3)\n",
      "LOG_DIR: log_test\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  EXTRA:\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "    STAGE2:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4]\n",
      "      NUM_BRANCHES: 2\n",
      "      NUM_CHANNELS: [32, 64]\n",
      "      NUM_MODULES: 1\n",
      "    STAGE3:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4]\n",
      "      NUM_BRANCHES: 3\n",
      "      NUM_CHANNELS: [32, 64, 128]\n",
      "      NUM_MODULES: 4\n",
      "    STAGE4:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4, 4]\n",
      "      NUM_BRANCHES: 4\n",
      "      NUM_CHANNELS: [32, 64, 128, 256]\n",
      "      NUM_MODULES: 3\n",
      "  HEATMAP_SIZE: [64, 64]\n",
      "  IMAGE_SIZE: [256, 256]\n",
      "  INIT_WEIGHTS: True\n",
      "  NAME: pose_hrnet\n",
      "  NUM_JOINTS: 16\n",
      "  PRETRAINED: models/pytorch/imagenet/hrnet_w32-36af842e.pth\n",
      "  SIGMA: 2\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output_test\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 100\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 32\n",
      "  BBOX_THRE: 1.0\n",
      "  COCO_BBOX_FILE: \n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.1\n",
      "  IN_VIS_THRE: 0.0\n",
      "  MODEL_FILE: /mnt/models/HRNet/pose_mpii/pose_hrnet_w32_256x256.pth\n",
      "  NMS_THRE: 0.6\n",
      "  OKS_THRE: 0.5\n",
      "  POST_PROCESS: True\n",
      "  SHIFT_HEATMAP: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: False\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 32\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 210\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LR: 0.001\n",
      "  LR_FACTOR: 0.1\n",
      "  LR_STEP: [170, 200]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating output_test/mpii/pose_hrnet/w32_256x256_adam_lr1e-3\n",
      "=> creating log_test/mpii/pose_hrnet/w32_256x256_adam_lr1e-3_2019-10-31-02-56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> loading model from /mnt/models/HRNet/pose_mpii/pose_hrnet_w32_256x256.pth\n",
      "=> load 2958 samples\n",
      "Test: [0/24]\tTime 19.645 (19.645)\tLoss 0.0005 (0.0005)\tAccuracy 0.876 (0.876)\n",
      "| Arch | Head | Shoulder | Elbow | Wrist | Hip | Knee | Ankle | Mean | Mean@0.1 |\n",
      "|---|---|---|---|---|---|---|---|---|---|\n",
      "| pose_hrnet | 97.101 | 95.941 | 90.336 | 86.449 | 89.095 | 87.084 | 83.278 | 90.330 | 37.702 |\n"
     ]
    }
   ],
   "source": [
    "update_my_config()\n",
    "\n",
    "logger, output_dir, tb_log_dir = create_logger(\n",
    "    cfg, \"experiments/mpii/hrnet/w32_256x256_adam_lr1e-3.yaml\", 'valid')\n",
    "\n",
    "#logger.info(pprint.pformat(args))\n",
    "logger.info(cfg)\n",
    "    \n",
    "# cudnn related setting\n",
    "cudnn.benchmark = cfg.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n",
    "\n",
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=False\n",
    ")\n",
    "\n",
    "if cfg.TEST.MODEL_FILE:\n",
    "    logger.info('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "    model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=False)\n",
    "else:\n",
    "    model_state_file = os.path.join(\n",
    "        final_output_dir, 'final_state.pth'\n",
    "    )\n",
    "    logger.info('=> loading model from {}'.format(model_state_file))\n",
    "    model.load_state_dict(torch.load(model_state_file))\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=cfg.GPUS).cuda()\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = JointsMSELoss(\n",
    "    use_target_weight=cfg.LOSS.USE_TARGET_WEIGHT\n",
    ").cuda()\n",
    "\n",
    "# Data loading code\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "val_dataset = eval('dataset.'+cfg.DATASET.DATASET)(\n",
    "    cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, False,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.TEST.BATCH_SIZE_PER_GPU*len(cfg.GPUS),\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# evaluate on validation set\n",
    "# my_validate(val_loader, val_dataset, model, criterion,\n",
    "#             output_dir, tb_log_dir)\n",
    "\n",
    "# switch to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    all_preds, all_boxes, losses,  acc, image_path, filenames, imgnums = predict_batch(val_loader, val_dataset, model, criterion, output_dir)\n",
    "\n",
    "    name_values, perf_indicator = val_dataset.evaluate(\n",
    "        cfg, all_preds, output_dir, all_boxes, image_path,\n",
    "        filenames, imgnums\n",
    "    )\n",
    "        \n",
    "model_name = cfg.MODEL.NAME\n",
    "if isinstance(name_values, list):\n",
    "    for name_value in name_values:\n",
    "        _print_name_value(name_value, model_name)\n",
    "else:\n",
    "    _print_name_value(name_values, model_name)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2958, 16, 3)\n",
      "(2958, 6)\n",
      "2958\n"
     ]
    }
   ],
   "source": [
    "print(all_preds.shape)\n",
    "print(all_boxes.shape)\n",
    "print(len(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
