{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tools import init_paths\n",
    "\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger\n",
    "\n",
    "import dataset\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying validate function\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from core.function import AverageMeter\n",
    "from core.function import _print_name_value\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils.transforms import flip_back\n",
    "from core.evaluate import accuracy\n",
    "from core.inference import get_final_preds\n",
    "from utils.vis import save_debug_images\n",
    "\n",
    "\n",
    "def my_validate(val_loader, val_dataset, model, criterion, output_dir,\n",
    "             tb_log_dir, writer_dict=None):\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        all_preds, all_boxes, losses,  acc, image_path, filenames, imgnums = predict_batch(val_loader,val_dataset,model,criterion,output_dir)\n",
    "\n",
    "        name_values, perf_indicator = val_dataset.evaluate(\n",
    "            cfg, all_preds, output_dir, all_boxes, image_path,\n",
    "            filenames, imgnums\n",
    "        )\n",
    "\n",
    "        model_name = cfg.MODEL.NAME\n",
    "        if isinstance(name_values, list):\n",
    "            for name_value in name_values:\n",
    "                _print_name_value(name_value, model_name)\n",
    "        else:\n",
    "            _print_name_value(name_values, model_name)\n",
    "\n",
    "        if writer_dict:\n",
    "            writer = writer_dict['writer']\n",
    "            global_steps = writer_dict['valid_global_steps']\n",
    "            writer.add_scalar(\n",
    "                'valid_loss',\n",
    "                losses.avg,\n",
    "                global_steps\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                'valid_acc',\n",
    "                acc.avg,\n",
    "                global_steps\n",
    "            )\n",
    "            if isinstance(name_values, list):\n",
    "                for name_value in name_values:\n",
    "                    writer.add_scalars(\n",
    "                        'valid',\n",
    "                        dict(name_value),\n",
    "                        global_steps\n",
    "                    )\n",
    "            else:\n",
    "                writer.add_scalars(\n",
    "                    'valid',\n",
    "                    dict(name_values),\n",
    "                    global_steps\n",
    "                )\n",
    "            writer_dict['valid_global_steps'] = global_steps + 1\n",
    "\n",
    "    return perf_indicator\n",
    "\n",
    "\n",
    "def predict_batch(val_loader, val_dataset, model, criterion, output_dir):\n",
    "    end = time.time()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    num_samples = len(val_dataset)\n",
    "    all_preds = np.zeros(\n",
    "            (num_samples, cfg.MODEL.NUM_JOINTS, 3),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    all_boxes = np.zeros((num_samples, 6))\n",
    "    image_path = []\n",
    "    filenames = []\n",
    "    imgnums = []\n",
    "    idx = 0\n",
    "    for i, (input, target, target_weight, meta) in enumerate(val_loader):\n",
    "            # compute output\n",
    "            outputs = model(input)\n",
    "            if isinstance(outputs, list):\n",
    "                output = outputs[-1]\n",
    "            else:\n",
    "                output = outputs\n",
    "\n",
    "            if cfg.TEST.FLIP_TEST:\n",
    "                # this part is ugly, because pytorch has not supported negative index\n",
    "                # input_flipped = model(input[:, :, :, ::-1])\n",
    "                input_flipped = np.flip(input.cpu().numpy(), 3).copy()\n",
    "                input_flipped = torch.from_numpy(input_flipped).cuda()\n",
    "                outputs_flipped = model(input_flipped)\n",
    "\n",
    "                if isinstance(outputs_flipped, list):\n",
    "                    output_flipped = outputs_flipped[-1]\n",
    "                else:\n",
    "                    output_flipped = outputs_flipped\n",
    "\n",
    "                output_flipped = flip_back(output_flipped.cpu().numpy(),\n",
    "                                           val_dataset.flip_pairs)\n",
    "                output_flipped = torch.from_numpy(output_flipped.copy()).cuda()\n",
    "\n",
    "\n",
    "                # feature is not aligned, shift flipped heatmap for higher accuracy\n",
    "                if cfg.TEST.SHIFT_HEATMAP:\n",
    "                    output_flipped[:, :, :, 1:] = \\\n",
    "                        output_flipped.clone()[:, :, :, 0:-1]\n",
    "\n",
    "                output = (output + output_flipped) * 0.5\n",
    "\n",
    "            target = target.cuda(non_blocking=True)\n",
    "            target_weight = target_weight.cuda(non_blocking=True)\n",
    "\n",
    "            loss = criterion(output, target, target_weight)\n",
    "\n",
    "            num_images = input.size(0)\n",
    "            # measure accuracy and record loss\n",
    "            losses.update(loss.item(), num_images)\n",
    "            _, avg_acc, cnt, pred = accuracy(output.cpu().numpy(),\n",
    "                                             target.cpu().numpy())\n",
    "\n",
    "            acc.update(avg_acc, cnt)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            c = meta['center'].numpy()\n",
    "            s = meta['scale'].numpy()\n",
    "            score = meta['score'].numpy()\n",
    "\n",
    "            preds, maxvals = get_final_preds(\n",
    "                cfg, output.clone().cpu().numpy(), c, s)\n",
    "\n",
    "            all_preds[idx:idx + num_images, :, 0:2] = preds[:, :, 0:2]\n",
    "            all_preds[idx:idx + num_images, :, 2:3] = maxvals\n",
    "            # double check this all_boxes parts\n",
    "            all_boxes[idx:idx + num_images, 0:2] = c[:, 0:2]\n",
    "            all_boxes[idx:idx + num_images, 2:4] = s[:, 0:2]\n",
    "            all_boxes[idx:idx + num_images, 4] = np.prod(s*200, 1)\n",
    "            all_boxes[idx:idx + num_images, 5] = score\n",
    "            image_path.extend(meta['image'])\n",
    "\n",
    "            idx += num_images\n",
    "\n",
    "            if i % cfg.PRINT_FREQ == 0:\n",
    "                msg = 'Test: [{0}/{1}]\\t' \\\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                      'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                          i, len(val_loader), batch_time=batch_time,\n",
    "                          loss=losses, acc=acc)\n",
    "                logger.info(msg)\n",
    "\n",
    "                prefix = '{}_{}'.format(\n",
    "                    os.path.join(output_dir, 'val'), i\n",
    "                )\n",
    "                save_debug_images(cfg, input, meta, target, pred*4, output,\n",
    "                                  prefix)\n",
    "    return all_preds, all_boxes, losses,  acc, image_path, filenames, imgnums \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_my_config():\n",
    "    cfg.defrost()\n",
    "\n",
    "    cfg.merge_from_file('experiments/mpii/hrnet/w32_256x256_adam_lr1e-3.yaml')\n",
    "\n",
    "    opts = [\"TEST.MODEL_FILE\", \"models/HRNet/pose_mpii/pose_hrnet_w32_256x256.pth\"]\n",
    "#     opts = [\"TEST.MODEL_FILE\", \"/mnt/models/HRNet/pose_mpii/pose_hrnet_w32_256x256.pth\"]\n",
    "    cfg.merge_from_list(opts)\n",
    "\n",
    "    cfg.OUTPUT_DIR = \"output_test\"\n",
    "\n",
    "    cfg.LOG_DIR = \"log_test\"\n",
    "\n",
    "    cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: mpii\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: -1.0\n",
      "  ROOT: data/mpii/\n",
      "  ROT_FACTOR: 30\n",
      "  SCALE_FACTOR: 0.25\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: valid\n",
      "  TRAIN_SET: train\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: True\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0, 1, 2, 3)\n",
      "LOG_DIR: log_test\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  EXTRA:\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "    STAGE2:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4]\n",
      "      NUM_BRANCHES: 2\n",
      "      NUM_CHANNELS: [32, 64]\n",
      "      NUM_MODULES: 1\n",
      "    STAGE3:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4]\n",
      "      NUM_BRANCHES: 3\n",
      "      NUM_CHANNELS: [32, 64, 128]\n",
      "      NUM_MODULES: 4\n",
      "    STAGE4:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4, 4]\n",
      "      NUM_BRANCHES: 4\n",
      "      NUM_CHANNELS: [32, 64, 128, 256]\n",
      "      NUM_MODULES: 3\n",
      "  HEATMAP_SIZE: [64, 64]\n",
      "  IMAGE_SIZE: [256, 256]\n",
      "  INIT_WEIGHTS: True\n",
      "  NAME: pose_hrnet\n",
      "  NUM_JOINTS: 16\n",
      "  PRETRAINED: models/pytorch/imagenet/hrnet_w32-36af842e.pth\n",
      "  SIGMA: 2\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output_test\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 100\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 32\n",
      "  BBOX_THRE: 1.0\n",
      "  COCO_BBOX_FILE: \n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.1\n",
      "  IN_VIS_THRE: 0.0\n",
      "  MODEL_FILE: models/HRNet/pose_mpii/pose_hrnet_w32_256x256.pth\n",
      "  NMS_THRE: 0.6\n",
      "  OKS_THRE: 0.5\n",
      "  POST_PROCESS: True\n",
      "  SHIFT_HEATMAP: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: False\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 32\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 210\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LR: 0.001\n",
      "  LR_FACTOR: 0.1\n",
      "  LR_STEP: [170, 200]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 24\n",
      "=> loading model from models/HRNet/pose_mpii/pose_hrnet_w32_256x256.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating output_test/mpii/pose_hrnet/w32_256x256_adam_lr1e-3\n",
      "=> creating log_test/mpii/pose_hrnet/w32_256x256_adam_lr1e-3_2020-06-24-11-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> load 2958 samples\n",
      "Test: [0/24]\tTime 8.421 (8.421)\tLoss 0.0005 (0.0005)\tAccuracy 0.876 (0.876)\n",
      "| Arch | Head | Shoulder | Elbow | Wrist | Hip | Knee | Ankle | Mean | Mean@0.1 |\n",
      "|---|---|---|---|---|---|---|---|---|---|\n",
      "| pose_hrnet | 97.101 | 95.941 | 90.336 | 86.449 | 89.095 | 87.084 | 83.278 | 90.330 | 37.702 |\n"
     ]
    }
   ],
   "source": [
    "update_my_config()\n",
    "\n",
    "logger, output_dir, tb_log_dir = create_logger(\n",
    "    cfg, \"experiments/mpii/hrnet/w32_256x256_adam_lr1e-3.yaml\", 'valid')\n",
    "\n",
    "#logger.info(pprint.pformat(args))\n",
    "logger.info(cfg)\n",
    "    \n",
    "# cudnn related setting\n",
    "cudnn.benchmark = cfg.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n",
    "\n",
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=False\n",
    ")\n",
    "\n",
    "if cfg.TEST.MODEL_FILE:\n",
    "    logger.info('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "    model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=False)\n",
    "else:\n",
    "    model_state_file = os.path.join(\n",
    "        final_output_dir, 'final_state.pth'\n",
    "    )\n",
    "    logger.info('=> loading model from {}'.format(model_state_file))\n",
    "    model.load_state_dict(torch.load(model_state_file))\n",
    "\n",
    "    \n",
    "model = torch.nn.DataParallel(model, device_ids=[0]).cuda()\n",
    "# model = torch.nn.DataParallel(model, device_ids=cfg.GPUS).cuda()\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = JointsMSELoss(\n",
    "    use_target_weight=cfg.LOSS.USE_TARGET_WEIGHT\n",
    ").cuda()\n",
    "\n",
    "# Data loading code\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "val_dataset = eval('dataset.'+cfg.DATASET.DATASET)(\n",
    "    cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, False,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.TEST.BATCH_SIZE_PER_GPU*len(cfg.GPUS),\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# evaluate on validation set\n",
    "# my_validate(val_loader, val_dataset, model, criterion,\n",
    "#             output_dir, tb_log_dir)\n",
    "\n",
    "# switch to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    all_preds, all_boxes, losses,  acc, image_path, filenames, imgnums = predict_batch(val_loader, val_dataset, model, criterion, output_dir)\n",
    "\n",
    "    name_values, perf_indicator = val_dataset.evaluate(\n",
    "        cfg, all_preds, output_dir, all_boxes, image_path,\n",
    "        filenames, imgnums\n",
    "    )\n",
    "        \n",
    "model_name = cfg.MODEL.NAME\n",
    "if isinstance(name_values, list):\n",
    "    for name_value in name_values:\n",
    "        _print_name_value(name_value, model_name)\n",
    "else:\n",
    "    _print_name_value(name_values, model_name)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2958, 16, 3)\n",
      "(2958, 6)\n",
      "2958\n"
     ]
    }
   ],
   "source": [
    "print(all_preds.shape)\n",
    "print(all_boxes.shape)\n",
    "print(len(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing results\n",
    "`lib/dataset/mpii.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "from dataset.JointsDataset import JointsDataset\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def my_evaluate(cfg, preds, output_dir, *args, **kwargs):\n",
    "    # convert 0-based index to 1-based index\n",
    "    preds = preds[:, :, 0:2] + 1.0\n",
    "\n",
    "    if output_dir:\n",
    "        pred_file = os.path.join(output_dir, 'pred.mat')\n",
    "        savemat(pred_file, mdict={'preds': preds})\n",
    "\n",
    "    if 'test' in cfg.DATASET.TEST_SET:\n",
    "        return {'Null': 0.0}, 0.0\n",
    "\n",
    "    SC_BIAS = 0.6\n",
    "    threshold = 0.5\n",
    "\n",
    "    gt_file = os.path.join(cfg.DATASET.ROOT,\n",
    "                           'annot',\n",
    "                           'gt_{}.mat'.format(cfg.DATASET.TEST_SET))\n",
    "    gt_dict = loadmat(gt_file)\n",
    "    dataset_joints = gt_dict['dataset_joints']\n",
    "    jnt_missing = gt_dict['jnt_missing']\n",
    "    pos_gt_src = gt_dict['pos_gt_src']\n",
    "    headboxes_src = gt_dict['headboxes_src']\n",
    "\n",
    "    pos_pred_src = np.transpose(preds, [1, 2, 0])\n",
    "\n",
    "    head = np.where(dataset_joints == 'head')[1][0]\n",
    "    lsho = np.where(dataset_joints == 'lsho')[1][0]\n",
    "    lelb = np.where(dataset_joints == 'lelb')[1][0]\n",
    "    lwri = np.where(dataset_joints == 'lwri')[1][0]\n",
    "    lhip = np.where(dataset_joints == 'lhip')[1][0]\n",
    "    lkne = np.where(dataset_joints == 'lkne')[1][0]\n",
    "    lank = np.where(dataset_joints == 'lank')[1][0]\n",
    "\n",
    "    rsho = np.where(dataset_joints == 'rsho')[1][0]\n",
    "    relb = np.where(dataset_joints == 'relb')[1][0]\n",
    "    rwri = np.where(dataset_joints == 'rwri')[1][0]\n",
    "    rkne = np.where(dataset_joints == 'rkne')[1][0]\n",
    "    rank = np.where(dataset_joints == 'rank')[1][0]\n",
    "    rhip = np.where(dataset_joints == 'rhip')[1][0]\n",
    "\n",
    "    jnt_visible = 1 - jnt_missing\n",
    "    uv_error = pos_pred_src - pos_gt_src\n",
    "    uv_err = np.linalg.norm(uv_error, axis=1)\n",
    "    headsizes = headboxes_src[1, :, :] - headboxes_src[0, :, :]\n",
    "    headsizes = np.linalg.norm(headsizes, axis=0)\n",
    "    headsizes *= SC_BIAS\n",
    "    scale = np.multiply(headsizes, np.ones((len(uv_err), 1)))\n",
    "    scaled_uv_err = np.divide(uv_err, scale)\n",
    "    scaled_uv_err = np.multiply(scaled_uv_err, jnt_visible)\n",
    "    jnt_count = np.sum(jnt_visible, axis=1)\n",
    "    less_than_threshold = np.multiply((scaled_uv_err <= threshold),\n",
    "                                      jnt_visible)\n",
    "    PCKh = np.divide(100.*np.sum(less_than_threshold, axis=1), jnt_count)\n",
    "\n",
    "    # save\n",
    "    rng = np.arange(0, 0.5+0.01, 0.01)\n",
    "    pckAll = np.zeros((len(rng), 16))\n",
    "\n",
    "    for r in range(len(rng)):\n",
    "        threshold = rng[r]\n",
    "        less_than_threshold = np.multiply(scaled_uv_err <= threshold,\n",
    "                                          jnt_visible)\n",
    "        pckAll[r, :] = np.divide(100.*np.sum(less_than_threshold, axis=1),\n",
    "                                 jnt_count)\n",
    "\n",
    "    PCKh = np.ma.array(PCKh, mask=False)\n",
    "    PCKh.mask[6:8] = True\n",
    "\n",
    "    jnt_count = np.ma.array(jnt_count, mask=False)\n",
    "    jnt_count.mask[6:8] = True\n",
    "    jnt_ratio = jnt_count / np.sum(jnt_count).astype(np.float64)\n",
    "\n",
    "    name_value = [\n",
    "        ('Head', PCKh[head]),\n",
    "        ('Shoulder', 0.5 * (PCKh[lsho] + PCKh[rsho])),\n",
    "        ('Elbow', 0.5 * (PCKh[lelb] + PCKh[relb])),\n",
    "        ('Wrist', 0.5 * (PCKh[lwri] + PCKh[rwri])),\n",
    "        ('Hip', 0.5 * (PCKh[lhip] + PCKh[rhip])),\n",
    "        ('Knee', 0.5 * (PCKh[lkne] + PCKh[rkne])),\n",
    "        ('Ankle', 0.5 * (PCKh[lank] + PCKh[rank])),\n",
    "        ('Mean', np.sum(PCKh * jnt_ratio)),\n",
    "        ('Mean@0.1', np.sum(pckAll[11, :] * jnt_ratio))\n",
    "    ]\n",
    "    name_value = OrderedDict(name_value)\n",
    "\n",
    "    return name_value, name_value['Mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Head', 97.10095497953616), ('Shoulder', 95.94089673913044), ('Elbow', 90.33585228844147), ('Wrist', 86.44887639077518), ('Hip', 89.09468549632814), ('Knee', 87.0839159249281), ('Ankle', 83.27820897720389), ('Mean', 90.3304709862087), ('Mean@0.1', 37.70231589903722)]) 90.3304709862087\n"
     ]
    }
   ],
   "source": [
    "name_values, perf_indicator = my_evaluate(cfg, all_preds, output_dir, all_boxes, image_path, filenames, imgnums)\n",
    "print(name_values, perf_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2, 2958)\n"
     ]
    }
   ],
   "source": [
    "## get GT info\n",
    "gt_file = os.path.join(cfg.DATASET.ROOT, 'annot', 'gt_{}.mat'.format(cfg.DATASET.TEST_SET))\n",
    "gt_dict = loadmat(gt_file)\n",
    "dataset_joints = gt_dict['dataset_joints']\n",
    "jnt_missing = gt_dict['jnt_missing']\n",
    "pos_gt_src = gt_dict['pos_gt_src']\n",
    "print(pos_gt_src.shape)\n",
    "headboxes_src = gt_dict['headboxes_src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array(['rank'], dtype='<U4') array(['rkne'], dtype='<U4')\n",
      "  array(['rhip'], dtype='<U4') array(['lhip'], dtype='<U4')\n",
      "  array(['lkne'], dtype='<U4') array(['lank'], dtype='<U4')\n",
      "  array(['pelv'], dtype='<U4') array(['thor'], dtype='<U4')\n",
      "  array(['neck'], dtype='<U4') array(['head'], dtype='<U4')\n",
      "  array(['rwri'], dtype='<U4') array(['relb'], dtype='<U4')\n",
      "  array(['rsho'], dtype='<U4') array(['lsho'], dtype='<U4')\n",
      "  array(['lelb'], dtype='<U4') array(['lwri'], dtype='<U4')]]\n",
      "(array([0]), array([9]))\n"
     ]
    }
   ],
   "source": [
    "print(dataset_joints)\n",
    "print(np.where(dataset_joints == 'head'))\n",
    "# dataset_joints[0][9][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3, 2958)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pred_src = np.transpose(all_preds, [1, 2, 0])\n",
    "pos_pred_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sample = 1\n",
    "\n",
    "# read and plot image\n",
    "im = cv2.imread(image_path[idx_sample])\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(211)\n",
    "plt.imshow(im[:,:,::-1])\n",
    "\n",
    "# plot estimates joints\n",
    "plt.scatter(pos_pred_src[:,0,idx_sample], pos_pred_src[:,1,idx_sample])\n",
    "plt.scatter(pos_gt_src[:,0,idx_sample], pos_gt_src[:,1,idx_sample])\n",
    "plt.legend(['estimate', 'true'])\n",
    "\n",
    "# # plot head box\n",
    "# hbox = headboxes_src[:,:,idx_sample]\n",
    "\n",
    "# plot estiamte vs. true\n",
    "plt.subplot(223)\n",
    "plt.scatter(pos_gt_src[:,0,idx_sample], pos_pred_src[:,0,idx_sample])\n",
    "plt.xlabel('true x')\n",
    "plt.ylabel('estimated x')\n",
    "plt.axis('equal')\n",
    "plt.subplot(224)\n",
    "plt.scatter(pos_gt_src[:,1,idx_sample], pos_pred_src[:,1,idx_sample])\n",
    "plt.xlabel('true y')\n",
    "plt.ylabel('estimated y')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.savefig('result_{}.png'.format(idx_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preapre a scaled image\n",
    "from utils.transforms import get_affine_transform\n",
    "\n",
    "#center = np.array([im.shape[0]/2, im.shape[1]/2])\n",
    "center = np.array([320, 270])\n",
    "scale = 1.8\n",
    "rot = 0\n",
    "affine = get_affine_transform(center, scale, rot, [256,256])\n",
    "input = cv2.warpAffine(\n",
    "            im,\n",
    "            affine,\n",
    "            (256, 256),\n",
    "            flags=cv2.INTER_LINEAR)\n",
    "\n",
    "print(input.shape)\n",
    "plt.imshow(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "im_tensor = trans(input)\n",
    "print(im_tensor.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(im_tensor.reshape([1, 3, 256, 256]))\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.inference import get_final_preds\n",
    "\n",
    "pred, maxval = get_final_preds(cfg, output.clone().cpu().numpy(), [center], [scale])\n",
    "\n",
    "pred.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im[:,:,::-1])\n",
    "plt.scatter(pred[0,:,0], pred[0,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
